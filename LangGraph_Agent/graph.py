import os
from dotenv import load_dotenv 
from langchain_deepseek import ChatDeepSeek
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool
from pydantic import BaseModel, Field
import matplotlib
import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import pymysql
from langchain_tavily import TavilySearch
import requests
from bs4 import BeautifulSoup
import subprocess
import platform
import psutil
import datetime
from urllib.parse import quote
import time
import csv
from pathlib import Path

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(override=True)

# å¯¼å…¥é«˜å¾·åœ°å›¾å·¥å…·
from amap_tools import AMAP_TOOLS

# âœ… åˆ›å»ºTavilyæœç´¢å·¥å…·
search_tool = TavilySearch(max_results=5, topic="general")

# âœ… åˆ›å»ºè°·æ­Œæœç´¢å·¥å…·ï¼ˆä¸éœ€è¦API KEYï¼‰
class GoogleSearchSchema(BaseModel):
    query: str = Field(description="æœç´¢æŸ¥è¯¢å…³é”®è¯")
    num_results: int = Field(default=5, description="è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤5ä¸ª")

@tool(args_schema=GoogleSearchSchema)
def google_search(query: str, num_results: int = 5) -> str:
    """
    ä½¿ç”¨è°·æ­Œæœç´¢è·å–ä¿¡æ¯ï¼Œä¸éœ€è¦API KEYã€‚
    é€‚ç”¨äºæœç´¢æœ€æ–°ä¿¡æ¯ã€æ–°é—»ã€æŠ€æœ¯æ–‡æ¡£ç­‰ã€‚
    """
    try:
        # æ„å»ºæœç´¢URL
        search_url = f"https://www.google.com/search?q={quote(query)}&num={num_results}"
        
        # è®¾ç½®è¯·æ±‚å¤´æ¨¡æ‹Ÿæµè§ˆå™¨
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(search_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æœç´¢ç»“æœ
        results = []
        search_results = soup.find_all('div', class_='g')
        
        for i, result in enumerate(search_results[:num_results]):
            title_elem = result.find('h3')
            link_elem = result.find('a')
            snippet_elem = result.find('span', class_='aCOpRe')
            
            if title_elem and link_elem:
                title = title_elem.get_text()
                link = link_elem.get('href', '')
                snippet = snippet_elem.get_text() if snippet_elem else "æ— æè¿°"
                
                results.append({
                    'title': title,
                    'link': link,
                    'snippet': snippet
                })
        
        if results:
            return json.dumps(results, ensure_ascii=False, indent=2)
        else:
            return "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ"
            
    except Exception as e:
        return f"æœç´¢å¤±è´¥: {str(e)}"

# âœ… åˆ›å»ºç½‘é¡µå†…å®¹æŠ“å–å·¥å…·
class WebScrapingSchema(BaseModel):
    url: str = Field(description="è¦æŠ“å–çš„ç½‘é¡µURL")
    extract_text: bool = Field(default=True, description="æ˜¯å¦åªæå–æ–‡æœ¬å†…å®¹")

@tool(args_schema=WebScrapingSchema)
def web_scraping(url: str, extract_text: bool = True) -> str:
    """
    æŠ“å–æŒ‡å®šç½‘é¡µçš„å†…å®¹ã€‚
    å¯ä»¥è·å–ç½‘é¡µçš„æ–‡æœ¬å†…å®¹æˆ–HTMLæºç ã€‚
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        
        if extract_text:
            soup = BeautifulSoup(response.text, 'html.parser')
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
            for script in soup(["script", "style"]):
                script.decompose()
            
            # è·å–æ–‡æœ¬å†…å®¹
            text = soup.get_text()
            # æ¸…ç†æ–‡æœ¬
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)
            
            # é™åˆ¶è¿”å›å†…å®¹é•¿åº¦
            if len(text) > 3000:
                text = text[:3000] + "\n\n[å†…å®¹å·²æˆªæ–­...]"
            
            return text
        else:
            return response.text[:3000] + "\n\n[HTMLå†…å®¹å·²æˆªæ–­...]" if len(response.text) > 3000 else response.text
            
    except Exception as e:
         return f"ç½‘é¡µæŠ“å–å¤±è´¥: {str(e)}"

# âœ… åˆ›å»ºæ–‡ä»¶æ“ä½œå·¥å…·
class FileOperationSchema(BaseModel):
    operation: str = Field(description="æ“ä½œç±»å‹: read, write, append, list, delete, exists")
    file_path: str = Field(description="æ–‡ä»¶è·¯å¾„")
    content: str = Field(default="", description="å†™å…¥çš„å†…å®¹ï¼ˆä»…ç”¨äºwriteå’Œappendæ“ä½œï¼‰")

@tool(args_schema=FileOperationSchema)
def file_operations(operation: str, file_path: str, content: str = "") -> str:
    """
    æ‰§è¡Œæ–‡ä»¶æ“ä½œï¼ŒåŒ…æ‹¬è¯»å–ã€å†™å…¥ã€è¿½åŠ ã€åˆ—å‡ºç›®å½•ã€åˆ é™¤æ–‡ä»¶ã€æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚
    æ”¯æŒçš„æ“ä½œç±»å‹ï¼šread, write, append, list, delete, exists
    """
    try:
        path = Path(file_path)
        
        if operation == "read":
            if path.exists() and path.is_file():
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if len(content) > 2000:
                        content = content[:2000] + "\n\n[æ–‡ä»¶å†…å®¹å·²æˆªæ–­...]"
                    return content
            else:
                return f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                
        elif operation == "write":
            path.parent.mkdir(parents=True, exist_ok=True)
            with open(path, 'w', encoding='utf-8') as f:
                f.write(content)
            return f"âœ… æ–‡ä»¶å·²å†™å…¥: {file_path}"
            
        elif operation == "append":
            path.parent.mkdir(parents=True, exist_ok=True)
            with open(path, 'a', encoding='utf-8') as f:
                f.write(content)
            return f"âœ… å†…å®¹å·²è¿½åŠ åˆ°æ–‡ä»¶: {file_path}"
            
        elif operation == "list":
            if path.exists() and path.is_dir():
                items = []
                for item in path.iterdir():
                    item_type = "ç›®å½•" if item.is_dir() else "æ–‡ä»¶"
                    size = item.stat().st_size if item.is_file() else "-"
                    items.append(f"{item_type}: {item.name} ({size} bytes)")
                return "\n".join(items[:50])  # é™åˆ¶æ˜¾ç¤º50ä¸ªé¡¹ç›®
            else:
                return f"ç›®å½•ä¸å­˜åœ¨: {file_path}"
                
        elif operation == "delete":
            if path.exists():
                if path.is_file():
                    path.unlink()
                    return f"âœ… æ–‡ä»¶å·²åˆ é™¤: {file_path}"
                else:
                    return f"âŒ æ— æ³•åˆ é™¤ç›®å½•ï¼Œè¯·ä½¿ç”¨ä¸“é—¨çš„ç›®å½•åˆ é™¤å·¥å…·"
            else:
                return f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                
        elif operation == "exists":
            return f"æ–‡ä»¶å­˜åœ¨: {path.exists()}"
            
        else:
            return f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {operation}"
            
    except Exception as e:
        return f"æ–‡ä»¶æ“ä½œå¤±è´¥: {str(e)}"

# âœ… åˆ›å»ºç³»ç»Ÿä¿¡æ¯å·¥å…·
@tool
def get_system_info() -> str:
    """
    è·å–å½“å‰ç³»ç»Ÿçš„åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ“ä½œç³»ç»Ÿã€CPUã€å†…å­˜ã€ç£ç›˜ä½¿ç”¨æƒ…å†µç­‰ã€‚
    """
    try:
        info = {
            "æ“ä½œç³»ç»Ÿ": platform.system(),
            "ç³»ç»Ÿç‰ˆæœ¬": platform.release(),
            "å¤„ç†å™¨": platform.processor(),
            "CPUæ ¸å¿ƒæ•°": psutil.cpu_count(logical=False),
            "é€»è¾‘CPUæ•°": psutil.cpu_count(logical=True),
            "CPUä½¿ç”¨ç‡": f"{psutil.cpu_percent(interval=1):.1f}%",
            "å†…å­˜æ€»é‡": f"{psutil.virtual_memory().total / (1024**3):.1f} GB",
            "å†…å­˜ä½¿ç”¨ç‡": f"{psutil.virtual_memory().percent:.1f}%",
            "ç£ç›˜ä½¿ç”¨ç‡": f"{psutil.disk_usage('/').percent:.1f}%",
            "å½“å‰æ—¶é—´": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return json.dumps(info, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {str(e)}"

# âœ… åˆ›å»ºå‘½ä»¤æ‰§è¡Œå·¥å…·
class CommandSchema(BaseModel):
    command: str = Field(description="è¦æ‰§è¡Œçš„å‘½ä»¤")
    timeout: int = Field(default=30, description="è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")

@tool(args_schema=CommandSchema)
def execute_command(command: str, timeout: int = 30) -> str:
    """
    æ‰§è¡Œç³»ç»Ÿå‘½ä»¤å¹¶è¿”å›ç»“æœã€‚
    æ³¨æ„ï¼šä»…æ‰§è¡Œå®‰å…¨çš„å‘½ä»¤ï¼Œé¿å…æ‰§è¡Œå¯èƒ½æŸå®³ç³»ç»Ÿçš„å‘½ä»¤ã€‚
    """
    try:
        # å®‰å…¨æ£€æŸ¥ï¼šç¦æ­¢æ‰§è¡Œå±é™©å‘½ä»¤
        dangerous_commands = ['rm -rf', 'del', 'format', 'fdisk', 'mkfs', 'dd if=', 'shutdown', 'reboot']
        if any(dangerous in command.lower() for dangerous in dangerous_commands):
            return "âŒ æ‹’ç»æ‰§è¡Œæ½œåœ¨å±é™©å‘½ä»¤"
        
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            timeout=timeout
        )
        
        output = result.stdout if result.stdout else result.stderr
        if len(output) > 2000:
            output = output[:2000] + "\n\n[è¾“å‡ºå·²æˆªæ–­...]"
            
        return f"å‘½ä»¤æ‰§è¡Œç»“æœ (è¿”å›ç : {result.returncode}):\n{output}"
        
    except subprocess.TimeoutExpired:
        return f"âŒ å‘½ä»¤æ‰§è¡Œè¶…æ—¶ ({timeout}ç§’)"
    except Exception as e:
        return f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥: {str(e)}"

# âœ… åˆ›å»ºæ—¶é—´å’Œæ—¥æœŸå·¥å…·
class DateTimeSchema(BaseModel):
    operation: str = Field(description="æ“ä½œç±»å‹: now, format, calculate, timezone")
    date_string: str = Field(default="", description="æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆç”¨äºæ ¼å¼åŒ–æˆ–è®¡ç®—ï¼‰")
    format_string: str = Field(default="%Y-%m-%d %H:%M:%S", description="æ—¥æœŸæ ¼å¼")
    days_offset: int = Field(default=0, description="å¤©æ•°åç§»é‡ï¼ˆç”¨äºæ—¥æœŸè®¡ç®—ï¼‰")

@tool(args_schema=DateTimeSchema)
def datetime_operations(operation: str, date_string: str = "", format_string: str = "%Y-%m-%d %H:%M:%S", days_offset: int = 0) -> str:
    """
    æ‰§è¡Œæ—¥æœŸæ—¶é—´ç›¸å…³æ“ä½œï¼ŒåŒ…æ‹¬è·å–å½“å‰æ—¶é—´ã€æ ¼å¼åŒ–æ—¥æœŸã€æ—¥æœŸè®¡ç®—ç­‰ã€‚
    """
    try:
        if operation == "now":
            return datetime.datetime.now().strftime(format_string)
            
        elif operation == "format":
            if date_string:
                # å°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸²
                dt = datetime.datetime.fromisoformat(date_string.replace('Z', '+00:00'))
                return dt.strftime(format_string)
            else:
                return "è¯·æä¾›è¦æ ¼å¼åŒ–çš„æ—¥æœŸå­—ç¬¦ä¸²"
                
        elif operation == "calculate":
            base_date = datetime.datetime.now()
            if date_string:
                base_date = datetime.datetime.fromisoformat(date_string.replace('Z', '+00:00'))
            
            new_date = base_date + datetime.timedelta(days=days_offset)
            return new_date.strftime(format_string)
            
        elif operation == "timezone":
            import time
            return f"å½“å‰æ—¶åŒº: {time.tzname[0]}, UTCåç§»: {time.timezone // 3600} å°æ—¶"
            
        else:
            return f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {operation}"
            
    except Exception as e:
        return f"æ—¥æœŸæ—¶é—´æ“ä½œå¤±è´¥: {str(e)}"

# âœ… åˆ›å»ºCSVæ•°æ®å¤„ç†å·¥å…·
class CSVOperationSchema(BaseModel):
    operation: str = Field(description="æ“ä½œç±»å‹: read, write, analyze")
    file_path: str = Field(description="CSVæ–‡ä»¶è·¯å¾„")
    data: str = Field(default="", description="CSVæ•°æ®ï¼ˆJSONæ ¼å¼ï¼Œç”¨äºå†™å…¥æ“ä½œï¼‰")

@tool(args_schema=CSVOperationSchema)
def csv_operations(operation: str, file_path: str, data: str = "") -> str:
    """
    æ‰§è¡ŒCSVæ–‡ä»¶æ“ä½œï¼ŒåŒ…æ‹¬è¯»å–ã€å†™å…¥ã€åŸºæœ¬åˆ†æã€‚
    """
    try:
        if operation == "read":
            df = pd.read_csv(file_path)
            # é™åˆ¶æ˜¾ç¤ºè¡Œæ•°
            if len(df) > 10:
                preview = df.head(10)
                return f"CSVæ–‡ä»¶é¢„è§ˆï¼ˆå‰10è¡Œï¼‰ï¼š\n{preview.to_string()}\n\næ€»è¡Œæ•°: {len(df)}, æ€»åˆ—æ•°: {len(df.columns)}"
            else:
                return f"CSVæ–‡ä»¶å†…å®¹ï¼š\n{df.to_string()}"
                
        elif operation == "write":
            if data:
                import json
                json_data = json.loads(data)
                df = pd.DataFrame(json_data)
                df.to_csv(file_path, index=False)
                return f"âœ… æ•°æ®å·²å†™å…¥CSVæ–‡ä»¶: {file_path}"
            else:
                return "è¯·æä¾›è¦å†™å…¥çš„æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰"
                
        elif operation == "analyze":
            df = pd.read_csv(file_path)
            analysis = {
                "è¡Œæ•°": len(df),
                "åˆ—æ•°": len(df.columns),
                "åˆ—å": list(df.columns),
                "æ•°æ®ç±»å‹": df.dtypes.to_dict(),
                "ç¼ºå¤±å€¼": df.isnull().sum().to_dict(),
                "æ•°å€¼åˆ—ç»Ÿè®¡": df.describe().to_dict() if len(df.select_dtypes(include=['number']).columns) > 0 else "æ— æ•°å€¼åˆ—"
            }
            return json.dumps(analysis, ensure_ascii=False, indent=2, default=str)
            
        else:
            return f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {operation}"
            
    except Exception as e:
        return f"CSVæ“ä½œå¤±è´¥: {str(e)}"

# âœ… åˆ›å»ºSQLæŸ¥è¯¢å·¥å…·
description = """
å½“ç”¨æˆ·éœ€è¦è¿›è¡Œæ•°æ®åº“æŸ¥è¯¢å·¥ä½œæ—¶ï¼Œè¯·è°ƒç”¨è¯¥å‡½æ•°ã€‚
è¯¥å‡½æ•°ç”¨äºåœ¨æŒ‡å®šMySQLæœåŠ¡å™¨ä¸Šè¿è¡Œä¸€æ®µSQLä»£ç ï¼Œå®Œæˆæ•°æ®æŸ¥è¯¢ç›¸å…³å·¥ä½œï¼Œ
å¹¶ä¸”å½“å‰å‡½æ•°æ˜¯ä½¿ç”¨pymsqlè¿æ¥MySQLæ•°æ®åº“ã€‚
æœ¬å‡½æ•°åªè´Ÿè´£è¿è¡ŒSQLä»£ç å¹¶è¿›è¡Œæ•°æ®æŸ¥è¯¢ï¼Œè‹¥è¦è¿›è¡Œæ•°æ®æå–ï¼Œåˆ™ä½¿ç”¨å¦ä¸€ä¸ªextract_dataå‡½æ•°ã€‚
"""

# å®šä¹‰ç»“æ„åŒ–å‚æ•°æ¨¡å‹
class SQLQuerySchema(BaseModel):
    sql_query: str = Field(description=description)

# å°è£…ä¸º LangGraph å·¥å…·
@tool(args_schema=SQLQuerySchema)
def sql_inter(sql_query: str) -> str:
    """
    å½“ç”¨æˆ·éœ€è¦è¿›è¡Œæ•°æ®åº“æŸ¥è¯¢å·¥ä½œæ—¶ï¼Œè¯·è°ƒç”¨è¯¥å‡½æ•°ã€‚
    è¯¥å‡½æ•°ç”¨äºåœ¨æŒ‡å®šMySQLæœåŠ¡å™¨ä¸Šè¿è¡Œä¸€æ®µSQLä»£ç ï¼Œå®Œæˆæ•°æ®æŸ¥è¯¢ç›¸å…³å·¥ä½œï¼Œ
    å¹¶ä¸”å½“å‰å‡½æ•°æ˜¯ä½¿ç”¨pymsqlè¿æ¥MySQLæ•°æ®åº“ã€‚
    æœ¬å‡½æ•°åªè´Ÿè´£è¿è¡ŒSQLä»£ç å¹¶è¿›è¡Œæ•°æ®æŸ¥è¯¢ï¼Œè‹¥è¦è¿›è¡Œæ•°æ®æå–ï¼Œåˆ™ä½¿ç”¨å¦ä¸€ä¸ªextract_dataå‡½æ•°ã€‚
    :param sql_query: å­—ç¬¦ä¸²å½¢å¼çš„SQLæŸ¥è¯¢è¯­å¥ï¼Œç”¨äºæ‰§è¡Œå¯¹MySQLä¸­telco_dbæ•°æ®åº“ä¸­å„å¼ è¡¨è¿›è¡ŒæŸ¥è¯¢ï¼Œå¹¶è·å¾—å„è¡¨ä¸­çš„å„ç±»ç›¸å…³ä¿¡æ¯
    :returnï¼šsql_queryåœ¨MySQLä¸­çš„è¿è¡Œç»“æœã€‚   
    """
    # print("æ­£åœ¨è°ƒç”¨ sql_inter å·¥å…·è¿è¡Œ SQL æŸ¥è¯¢...")
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv(override=True)
    host = os.getenv('HOST')
    user = os.getenv('USER')
    mysql_pw = os.getenv('MYSQL_PW')
    db = os.getenv('DB_NAME')
    port = os.getenv('PORT')
    
    # åˆ›å»ºè¿æ¥
    connection = pymysql.connect(
        host=host,
        user=user,
        passwd=mysql_pw,
        db=db,
        port=int(port),
        charset='utf8'
    )
    
    try:
        with connection.cursor() as cursor:
            cursor.execute(sql_query)
            results = cursor.fetchall()
            # print("SQL æŸ¥è¯¢å·²æˆåŠŸæ‰§è¡Œï¼Œæ­£åœ¨æ•´ç†ç»“æœ...")
    finally:
        connection.close()

    # å°†ç»“æœä»¥ JSON å­—ç¬¦ä¸²å½¢å¼è¿”å›
    return json.dumps(results, ensure_ascii=False)

# âœ… åˆ›å»ºæ•°æ®æå–å·¥å…·
# å®šä¹‰ç»“æ„åŒ–å‚æ•°
class ExtractQuerySchema(BaseModel):
    sql_query: str = Field(description="ç”¨äºä» MySQL æå–æ•°æ®çš„ SQL æŸ¥è¯¢è¯­å¥ã€‚")
    df_name: str = Field(description="æŒ‡å®šç”¨äºä¿å­˜ç»“æœçš„ pandas å˜é‡åç§°ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰ã€‚")

# æ³¨å†Œä¸º Agent å·¥å…·
@tool(args_schema=ExtractQuerySchema)
def extract_data(sql_query: str, df_name: str) -> str:
    """
    ç”¨äºåœ¨MySQLæ•°æ®åº“ä¸­æå–ä¸€å¼ è¡¨åˆ°å½“å‰Pythonç¯å¢ƒä¸­ï¼Œæ³¨æ„ï¼Œæœ¬å‡½æ•°åªè´Ÿè´£æ•°æ®è¡¨çš„æå–ï¼Œ
    å¹¶ä¸è´Ÿè´£æ•°æ®æŸ¥è¯¢ï¼Œè‹¥éœ€è¦åœ¨MySQLä¸­è¿›è¡Œæ•°æ®æŸ¥è¯¢ï¼Œè¯·ä½¿ç”¨sql_interå‡½æ•°ã€‚
    åŒæ—¶éœ€è¦æ³¨æ„ï¼Œç¼–å†™å¤–éƒ¨å‡½æ•°çš„å‚æ•°æ¶ˆæ¯æ—¶ï¼Œå¿…é¡»æ˜¯æ»¡è¶³jsonæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œ
    :param sql_query: å­—ç¬¦ä¸²å½¢å¼çš„SQLæŸ¥è¯¢è¯­å¥ï¼Œç”¨äºæå–MySQLä¸­çš„æŸå¼ è¡¨ã€‚
    :param df_name: å°†MySQLæ•°æ®åº“ä¸­æå–çš„è¡¨æ ¼è¿›è¡Œæœ¬åœ°ä¿å­˜æ—¶çš„å˜é‡åï¼Œä»¥å­—ç¬¦ä¸²å½¢å¼è¡¨ç¤ºã€‚
    :returnï¼šè¡¨æ ¼è¯»å–å’Œä¿å­˜ç»“æœ
    """
    print("æ­£åœ¨è°ƒç”¨ extract_data å·¥å…·è¿è¡Œ SQL æŸ¥è¯¢...")
    
    load_dotenv(override=True)
    host = os.getenv('HOST')
    user = os.getenv('USER')
    mysql_pw = os.getenv('MYSQL_PW')
    db = os.getenv('DB_NAME')
    port = os.getenv('PORT')

    # åˆ›å»ºæ•°æ®åº“è¿æ¥
    connection = pymysql.connect(
        host=host,
        user=user,
        passwd=mysql_pw,
        db=db,
        port=int(port),
        charset='utf8'
    )

    try:
        # æ‰§è¡Œ SQL å¹¶ä¿å­˜ä¸ºå…¨å±€å˜é‡
        df = pd.read_sql(sql_query, connection)
        globals()[df_name] = df
        # print("æ•°æ®æˆåŠŸæå–å¹¶ä¿å­˜ä¸ºå…¨å±€å˜é‡ï¼š", df_name)
        return f"âœ… æˆåŠŸåˆ›å»º pandas å¯¹è±¡ `{df_name}`ï¼ŒåŒ…å«ä» MySQL æå–çš„æ•°æ®ã€‚"
    except Exception as e:
        return f"âŒ æ‰§è¡Œå¤±è´¥ï¼š{e}"
    finally:
        connection.close()

# âœ…åˆ›å»ºPythonä»£ç æ‰§è¡Œå·¥å…·
# Pythonä»£ç æ‰§è¡Œå·¥å…·ç»“æ„åŒ–å‚æ•°è¯´æ˜
class PythonCodeInput(BaseModel):
    py_code: str = Field(description="ä¸€æ®µåˆæ³•çš„ Python ä»£ç å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ '2 + 2' æˆ– 'x = 3\\ny = x * 2'")

@tool(args_schema=PythonCodeInput)
def python_inter(py_code):
    """
    å½“ç”¨æˆ·éœ€è¦ç¼–å†™Pythonç¨‹åºå¹¶æ‰§è¡Œæ—¶ï¼Œè¯·è°ƒç”¨è¯¥å‡½æ•°ã€‚
    è¯¥å‡½æ•°å¯ä»¥æ‰§è¡Œä¸€æ®µPythonä»£ç å¹¶è¿”å›æœ€ç»ˆç»“æœï¼Œéœ€è¦æ³¨æ„ï¼Œæœ¬å‡½æ•°åªèƒ½æ‰§è¡Œéç»˜å›¾ç±»çš„ä»£ç ï¼Œè‹¥æ˜¯ç»˜å›¾ç›¸å…³ä»£ç ï¼Œåˆ™éœ€è¦è°ƒç”¨fig_interå‡½æ•°è¿è¡Œã€‚
    """    
    g = globals()
    try:
        # å°è¯•å¦‚æœæ˜¯è¡¨è¾¾å¼ï¼Œåˆ™è¿”å›è¡¨è¾¾å¼è¿è¡Œç»“æœ
        return str(eval(py_code, g))
    # è‹¥æŠ¥é”™ï¼Œåˆ™å…ˆæµ‹è¯•æ˜¯å¦æ˜¯å¯¹ç›¸åŒå˜é‡é‡å¤èµ‹å€¼
    except Exception as e:
        global_vars_before = set(g.keys())
        try:            
            exec(py_code, g)
        except Exception as e:
            return f"ä»£ç æ‰§è¡Œæ—¶æŠ¥é”™{e}"
        global_vars_after = set(g.keys())
        new_vars = global_vars_after - global_vars_before
        # è‹¥å­˜åœ¨æ–°å˜é‡
        if new_vars:
            result = {var: g[var] for var in new_vars}
            # print("ä»£ç å·²é¡ºåˆ©æ‰§è¡Œï¼Œæ­£åœ¨è¿›è¡Œç»“æœæ¢³ç†...")
            return str(result)
        else:
            # print("ä»£ç å·²é¡ºåˆ©æ‰§è¡Œï¼Œæ­£åœ¨è¿›è¡Œç»“æœæ¢³ç†...")
            return "å·²ç»é¡ºåˆ©æ‰§è¡Œä»£ç "

# âœ… åˆ›å»ºç»˜å›¾å·¥å…·
# ç»˜å›¾å·¥å…·ç»“æ„åŒ–å‚æ•°è¯´æ˜
class FigCodeInput(BaseModel):
    py_code: str = Field(description="è¦æ‰§è¡Œçš„ Python ç»˜å›¾ä»£ç ï¼Œå¿…é¡»ä½¿ç”¨ matplotlib/seaborn åˆ›å»ºå›¾åƒå¹¶èµ‹å€¼ç»™å˜é‡")
    fname: str = Field(description="å›¾åƒå¯¹è±¡çš„å˜é‡åï¼Œä¾‹å¦‚ 'fig'ï¼Œç”¨äºä»ä»£ç ä¸­æå–å¹¶ä¿å­˜ä¸ºå›¾ç‰‡")

@tool(args_schema=FigCodeInput)
def fig_inter(py_code: str, fname: str) -> str:
    """
    å½“ç”¨æˆ·éœ€è¦ä½¿ç”¨ Python è¿›è¡Œå¯è§†åŒ–ç»˜å›¾ä»»åŠ¡æ—¶ï¼Œè¯·è°ƒç”¨è¯¥å‡½æ•°ã€‚

    æ³¨æ„ï¼š
    1. æ‰€æœ‰ç»˜å›¾ä»£ç å¿…é¡»åˆ›å»ºä¸€ä¸ªå›¾åƒå¯¹è±¡ï¼Œå¹¶å°†å…¶èµ‹å€¼ä¸ºæŒ‡å®šå˜é‡åï¼ˆä¾‹å¦‚ `fig`ï¼‰ã€‚
    2. å¿…é¡»ä½¿ç”¨ `fig = plt.figure()` æˆ– `fig = plt.subplots()`ã€‚
    3. ä¸è¦ä½¿ç”¨ `plt.show()`ã€‚
    4. è¯·ç¡®ä¿ä»£ç æœ€åè°ƒç”¨ `fig.tight_layout()`ã€‚
    5. æ‰€æœ‰ç»˜å›¾ä»£ç ä¸­ï¼Œåæ ‡è½´æ ‡ç­¾ï¼ˆxlabelã€ylabelï¼‰ã€æ ‡é¢˜ï¼ˆtitleï¼‰ã€å›¾ä¾‹ï¼ˆlegendï¼‰ç­‰æ–‡æœ¬å†…å®¹ï¼Œå¿…é¡»ä½¿ç”¨è‹±æ–‡æè¿°ã€‚

    ç¤ºä¾‹ä»£ç ï¼š
    fig = plt.figure(figsize=(10,6))
    plt.plot([1,2,3], [4,5,6])
    fig.tight_layout()
    """
    # print("æ­£åœ¨è°ƒç”¨fig_interå·¥å…·è¿è¡ŒPythonä»£ç ...")

    current_backend = matplotlib.get_backend()
    matplotlib.use('Agg')

    local_vars = {"plt": plt, "pd": pd, "sns": sns}
    
    # âœ… è®¾ç½®å›¾åƒä¿å­˜è·¯å¾„ï¼ˆåŠ¨æ€è·å–å½“å‰é¡¹ç›®ç›®å½•ï¼‰
    current_dir = os.path.dirname(os.path.abspath(__file__))  # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    images_dir = os.path.join(current_dir, "images")
    os.makedirs(images_dir, exist_ok=True)  # âœ… è‡ªåŠ¨åˆ›å»º images æ–‡ä»¶å¤¹ï¼ˆå¦‚ä¸å­˜åœ¨ï¼‰
    try:
        g = globals()
        exec(py_code, g, local_vars)
        g.update(local_vars)

        fig = local_vars.get(fname, None)
        if fig:
            image_filename = f"{fname}.png"
            abs_path = os.path.join(images_dir, image_filename)  # âœ… ç»å¯¹è·¯å¾„
            rel_path = os.path.join("images", image_filename)    # âœ… è¿”å›ç›¸å¯¹è·¯å¾„ï¼ˆç»™å‰ç«¯ç”¨ï¼‰

            fig.savefig(abs_path, bbox_inches='tight')
            return f"âœ… å›¾ç‰‡å·²ä¿å­˜ï¼Œè·¯å¾„ä¸º: {rel_path}"
        else:
            return "âš ï¸ å›¾åƒå¯¹è±¡æœªæ‰¾åˆ°ï¼Œè¯·ç¡®è®¤å˜é‡åæ­£ç¡®å¹¶ä¸º matplotlib å›¾å¯¹è±¡ã€‚"
    except Exception as e:
        return f"âŒ æ‰§è¡Œå¤±è´¥ï¼š{e}"
    finally:
        plt.close('all')
        matplotlib.use(current_backend)

# âœ… åˆ›å»ºæç¤ºè¯æ¨¡æ¿
prompt = """
ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œæ“…é•¿å¸®åŠ©ç”¨æˆ·é«˜æ•ˆå®Œæˆå„ç§ä»»åŠ¡ã€‚ä½ æ‹¥æœ‰ä»¥ä¸‹å¼ºå¤§çš„å·¥å…·èƒ½åŠ›ï¼š

## ğŸ” æœç´¢å’Œä¿¡æ¯è·å–å·¥å…·
1. **Tavilyæœç´¢ (search_tool)**: é«˜è´¨é‡çš„ç½‘ç»œæœç´¢ï¼Œé€‚åˆè·å–æœ€æ–°ä¿¡æ¯å’Œæ–°é—»
2. **è°·æ­Œæœç´¢ (google_search)**: å…è´¹çš„è°·æ­Œæœç´¢ï¼Œæ— éœ€API KEYï¼Œé€‚åˆä¸€èˆ¬ä¿¡æ¯æŸ¥è¯¢
3. **ç½‘é¡µæŠ“å– (web_scraping)**: æŠ“å–æŒ‡å®šç½‘é¡µçš„å†…å®¹ï¼Œå¯è·å–æ–‡æœ¬æˆ–HTMLæºç 

## ğŸ“ æ–‡ä»¶å’Œç³»ç»Ÿæ“ä½œå·¥å…·
4. **æ–‡ä»¶æ“ä½œ (file_operations)**: è¯»å–ã€å†™å…¥ã€è¿½åŠ ã€åˆ—å‡ºç›®å½•ã€åˆ é™¤æ–‡ä»¶ã€æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
5. **ç³»ç»Ÿä¿¡æ¯ (get_system_info)**: è·å–æ“ä½œç³»ç»Ÿã€CPUã€å†…å­˜ã€ç£ç›˜ç­‰ç³»ç»Ÿä¿¡æ¯
6. **å‘½ä»¤æ‰§è¡Œ (execute_command)**: å®‰å…¨åœ°æ‰§è¡Œç³»ç»Ÿå‘½ä»¤ï¼ˆç¦æ­¢å±é™©å‘½ä»¤ï¼‰
7. **æ—¶é—´æ—¥æœŸ (datetime_operations)**: è·å–å½“å‰æ—¶é—´ã€æ ¼å¼åŒ–æ—¥æœŸã€æ—¥æœŸè®¡ç®—ç­‰
8. **CSVå¤„ç† (csv_operations)**: è¯»å–ã€å†™å…¥ã€åˆ†æCSVæ–‡ä»¶

## ğŸ’¾ æ•°æ®åº“å’Œæ•°æ®åˆ†æå·¥å…·
9. **SQLæŸ¥è¯¢ (sql_inter)**: è¿æ¥MySQLæ•°æ®åº“æ‰§è¡ŒæŸ¥è¯¢ï¼Œå†…ç½®è¿æ¥å‚æ•°
10. **æ•°æ®æå– (extract_data)**: ä»MySQLæå–æ•°æ®åˆ°Python pandasç¯å¢ƒ
11. **Pythonæ‰§è¡Œ (python_inter)**: æ‰§è¡Œéç»˜å›¾ç±»Pythonä»£ç ï¼Œæ•°æ®å¤„ç†å’Œè®¡ç®—
12. **ç»˜å›¾å·¥å…· (fig_inter)**: æ‰§è¡Œmatplotlib/seabornç»˜å›¾ä»£ç å¹¶ä¿å­˜å›¾ç‰‡

## ğŸ—ºï¸ é«˜å¾·åœ°å›¾APIå·¥å…·
13. **åœ°ç†ç¼–ç  (amap_geocode)**: åœ°å€è½¬æ¢ä¸ºç»çº¬åº¦åæ ‡
14. **é€†åœ°ç†ç¼–ç  (amap_regeocode)**: åæ ‡è½¬æ¢ä¸ºè¯¦ç»†åœ°å€ä¿¡æ¯
15. **POIæœç´¢ (amap_poi_search)**: æœç´¢å…´è¶£ç‚¹ï¼Œå¦‚é¤å…ã€åŠ æ²¹ç«™ã€åŒ»é™¢ç­‰
16. **è¾“å…¥æç¤º (amap_input_tips)**: æœç´¢å»ºè®®å’Œè‡ªåŠ¨è¡¥å…¨
17. **é©¾è½¦è·¯å¾„è§„åˆ’ (amap_driving_route)**: é©¾è½¦å¯¼èˆªè·¯çº¿è§„åˆ’
18. **æ­¥è¡Œè·¯å¾„è§„åˆ’ (amap_walking_route)**: æ­¥è¡Œè·¯çº¿è§„åˆ’
19. **å¤©æ°”æŸ¥è¯¢ (amap_weather)**: å®æ—¶å¤©æ°”å’Œå¤©æ°”é¢„æŠ¥
20. **åæ ‡è½¬æ¢ (amap_coord_convert)**: ä¸åŒåæ ‡ç³»ä¹‹é—´çš„è½¬æ¢
21. **IPå®šä½ (amap_ip_location)**: æ ¹æ®IPåœ°å€è·å–ä½ç½®
22. **è¡Œæ”¿åŒºåŸŸæŸ¥è¯¢ (amap_district_search)**: æŸ¥è¯¢çœå¸‚åŒºè¡Œæ”¿åŒºåŸŸä¿¡æ¯

## ğŸ¯ å·¥å…·ä½¿ç”¨æŒ‡å—

**æœç´¢ä¿¡æ¯æ—¶ï¼š**
- æœ€æ–°æ–°é—»ã€å®æ—¶ä¿¡æ¯ â†’ ä½¿ç”¨ `search_tool` æˆ– `google_search`
- ç‰¹å®šç½‘é¡µå†…å®¹ â†’ ä½¿ç”¨ `web_scraping`

**æ–‡ä»¶æ“ä½œæ—¶ï¼š**
- è¯»å†™æ–‡ä»¶ â†’ ä½¿ç”¨ `file_operations`
- å¤„ç†CSVæ•°æ® â†’ ä½¿ç”¨ `csv_operations`
- ç³»ç»Ÿä¿¡æ¯æŸ¥è¯¢ â†’ ä½¿ç”¨ `get_system_info`
- æ‰§è¡Œå‘½ä»¤ â†’ ä½¿ç”¨ `execute_command`ï¼ˆå®‰å…¨é™åˆ¶ï¼‰

**æ•°æ®åˆ†ææ—¶ï¼š**
- æ•°æ®åº“æŸ¥è¯¢ â†’ å…ˆç”¨ `sql_inter` æˆ– `extract_data`
- æ•°æ®å¤„ç† â†’ ä½¿ç”¨ `python_inter`
- æ•°æ®å¯è§†åŒ– â†’ ä½¿ç”¨ `fig_inter`ï¼ˆå¿…é¡»åˆ›å»ºfigå¯¹è±¡ï¼Œä¸è¦ç”¨plt.show()ï¼‰

**æ—¶é—´å¤„ç†æ—¶ï¼š**
- è·å–å½“å‰æ—¶é—´ã€æ—¥æœŸè®¡ç®— â†’ ä½¿ç”¨ `datetime_operations`

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

1. **ç»˜å›¾è¦æ±‚ï¼š**
   - å¿…é¡»ä½¿ç”¨ `fig = plt.figure()` æˆ– `fig, ax = plt.subplots()` åˆ›å»ºå›¾åƒå¯¹è±¡
   - ä¸è¦ä½¿ç”¨ `plt.show()`
   - å›¾è¡¨æ ‡ç­¾å’Œæ ‡é¢˜ä½¿ç”¨è‹±æ–‡
   - è°ƒç”¨ `fig.tight_layout()`

2. **å®‰å…¨é™åˆ¶ï¼š**
   - å‘½ä»¤æ‰§è¡Œå·¥å…·ç¦æ­¢å±é™©å‘½ä»¤ï¼ˆå¦‚rm -rf, formatç­‰ï¼‰
   - æ–‡ä»¶æ“ä½œé™åˆ¶åœ¨å®‰å…¨èŒƒå›´å†…

3. **è¾“å‡ºæ ¼å¼ï¼š**
   - ä½¿ç”¨**ç®€ä½“ä¸­æ–‡**å›ç­”
   - JSONæ•°æ®è¦æå–å…³é”®ä¿¡æ¯ç®€è¦è¯´æ˜
   - ç”Ÿæˆå›¾ç‰‡æ—¶ä½¿ç”¨Markdownæ ¼å¼ï¼š`![æè¿°](images/å›¾ç‰‡å.png)`
   - ä¿æŒä¸“ä¸šã€ç®€æ´çš„é£æ ¼

4. **å·¥å…·é€‰æ‹©ä¼˜å…ˆçº§ï¼š**
   - æ•°æ®åº“æ“ä½œ â†’ SQLå·¥å…· â†’ Pythonåˆ†æ â†’ ç»˜å›¾
   - ä¿¡æ¯æŸ¥è¯¢ â†’ æœç´¢å·¥å…· â†’ ç½‘é¡µæŠ“å–
   - æ–‡ä»¶å¤„ç† â†’ ä¸“ç”¨æ–‡ä»¶å·¥å…· â†’ é€šç”¨Pythonå·¥å…·

è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ï¼Œæä¾›å‡†ç¡®ã€é«˜æ•ˆçš„å¸®åŠ©ã€‚
"""

# âœ… åˆ›å»ºå·¥å…·åˆ—è¡¨
tools = [
    search_tool, 
    google_search,
    web_scraping,
    file_operations,
    get_system_info,
    execute_command,
    datetime_operations,
    csv_operations,
    python_inter, 
    fig_inter, 
    sql_inter, 
    extract_data
] + AMAP_TOOLS  # æ·»åŠ é«˜å¾·åœ°å›¾å·¥å…·

# âœ… åˆ›å»ºæ¨¡å‹
model = ChatDeepSeek(model="deepseek-chat")

# âœ… åˆ›å»ºå›¾ ï¼ˆAgentï¼‰
graph = create_react_agent(model=model, tools=tools, prompt=prompt)